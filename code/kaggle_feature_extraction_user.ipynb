{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15d10fb",
   "metadata": {},
   "source": [
    "## JSON to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8d3d7",
   "metadata": {},
   "source": [
    "shb, reading all the data and generating one CSV for training and one for testing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a534eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from libs.edfa_feature_extraction_libs import featureExtraction_ML\n",
    "\n",
    "shb_train_dir = '../dataset/ML_challenge_user/Train/shb'\n",
    "shb_test_dir = '../dataset/ML_challenge_user/Test/shb'\n",
    "\n",
    "shb_output_csv_train = '../Features/Train/shb/preamp_random_features_train.csv'\n",
    "shb_output_csv_test = '../Features/Test/shb/preamp_random_features_test.csv'\n",
    "\n",
    "# ---- HARDCODED MAPPING ----\n",
    "edfa_name_to_index = {\n",
    "    'rdm1-co1': 0,\n",
    "    'rdm2-co1': 1,\n",
    "    'rdm3-co1': 2,\n",
    "    'rdm4-co1': 3,\n",
    "    'rdm5-co1': 4,\n",
    "    'rdm6-co1': 5,\n",
    "    'rdm1-lg1': 6,\n",
    "    'rdm2-lg1': 7\n",
    "}\n",
    "print(\"EDFA name to index mapping (hardcoded):\")\n",
    "for name, idx in edfa_name_to_index.items():\n",
    "    print(f\"  {name}: {idx}\")\n",
    "\n",
    "def extract_features_from_dir(data_dir, extractionType, channelType, featureType):\n",
    "    all_features = pd.DataFrame()\n",
    "    # print(sorted(os.listdir(data_dir)))\n",
    "    for fname in sorted(os.listdir(data_dir)):\n",
    "        # print(\"-----------------------\" + featureType + \": \" + fname)\n",
    "        if not fname.endswith('.json'):\n",
    "            continue\n",
    "        file_path = os.path.join(data_dir, fname)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if 'measurement_data' not in data or not data['measurement_data']:\n",
    "            print(f\"Skipping {fname}: no measurement_data\")\n",
    "            continue\n",
    "        \n",
    "        if 'preamp_' in fname:\n",
    "            edfa_name = fname.split('preamp_')[1].split('_')[0]\n",
    "        elif 'booster_' in fname:\n",
    "            edfa_name = fname.split('booster_')[1].split('_')[0]\n",
    "        else:\n",
    "            edfa_name = fname.split('_')[1] if len(fname.split('_')) > 1 else fname\n",
    "        features = featureExtraction_ML(\n",
    "            data['measurement_data'],\n",
    "            extractionType=extractionType,\n",
    "            channelType=channelType,\n",
    "            featureType=featureType,\n",
    "            channelNum=data['measurement_setup'].get('roadm_wss_num_channel', 95)\n",
    "        )\n",
    "        features['edfa_index'] = edfa_name_to_index.get(edfa_name, -1)  # -1 if not found\n",
    "        features['EDFA_type'] = extractionType\n",
    "        \n",
    "        # Move edfa_index to the first column\n",
    "        cols = features.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('EDFA_type')))\n",
    "        cols.insert(0, cols.pop(cols.index('edfa_index')))\n",
    "        features = features[cols]\n",
    "        all_features = pd.concat([all_features, features], ignore_index=True)\n",
    "        print(f\"Processed {fname}, edfa_name: {edfa_name}, edfa_index: {edfa_name_to_index.get(edfa_name, -1)}, rows: {len(features)}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error processing {fname}: {e}\")\n",
    "    return all_features\n",
    "\n",
    "# Extract features\n",
    "shb_train_features = extract_features_from_dir(\n",
    "    shb_train_dir, extractionType='preamp', channelType='random', featureType='train'\n",
    ")\n",
    "shb_test_features = extract_features_from_dir(\n",
    "    shb_test_dir, extractionType='preamp', channelType='random', featureType='test'\n",
    ")\n",
    "\n",
    "shb_train_features.to_csv(shb_output_csv_train, index=False)\n",
    "shb_test_features.to_csv(shb_output_csv_test, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d996a",
   "metadata": {},
   "source": [
    "Aging effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from libs.edfa_feature_extraction_libs import featureExtraction_ML\n",
    "\n",
    "aging_train_dir = '../dataset/ML_challenge_user/Train/aging'\n",
    "aging_test_dir = '../dataset/ML_challenge_user/Test/aging'\n",
    "\n",
    "aging_output_csv_train = '../Features/Train/aging/preamp_random_features_train.csv'\n",
    "aging_output_csv_test = '../Features/Test/aging/preamp_random_features_test.csv'\n",
    "\n",
    "# ---- HARDCODED MAPPING ----\n",
    "edfa_name_to_index = {\n",
    "    'rdm1-co1': 0,\n",
    "    'rdm2-co1': 1,\n",
    "    'rdm3-co1': 2,\n",
    "    'rdm4-co1': 3,\n",
    "    'rdm5-co1': 4,\n",
    "    'rdm6-co1': 5,\n",
    "    'rdm1-lg1': 6,\n",
    "    'rdm2-lg1': 7\n",
    "}\n",
    "print(\"EDFA name to index mapping (hardcoded):\")\n",
    "for name, idx in edfa_name_to_index.items():\n",
    "    print(f\"  {name}: {idx}\")\n",
    "\n",
    "def extract_features_from_dir(data_dir, featureType):\n",
    "    all_features = pd.DataFrame()\n",
    "    for fname in sorted(os.listdir(data_dir)):\n",
    "        if not fname.endswith('.json'):\n",
    "            continue\n",
    "        file_path = os.path.join(data_dir, fname)\n",
    "        print(file_path)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if 'measurement_data' not in data or not data['measurement_data']:\n",
    "            print(f\"Skipping {fname}: no measurement_data\")\n",
    "            continue\n",
    "\n",
    "        # Infer extractionType\n",
    "        if 'preamp' in fname:\n",
    "            extractionType_local = 'preamp'\n",
    "        elif 'booster' in fname:\n",
    "            extractionType_local = 'booster'\n",
    "        else:\n",
    "            print(f\"Skipping {fname}: cannot determine extractionType\")\n",
    "            continue\n",
    "\n",
    "        # Improved channelType detection\n",
    "        if '_fix_' in fname or '_goalpost_' in fname:\n",
    "            channelType_local = 'fix'\n",
    "        elif '_random_' in fname:\n",
    "            channelType_local = 'random'\n",
    "        elif '_extraRandom_' in fname:\n",
    "            channelType_local = 'extraRandom'\n",
    "        elif '_extraLow_' in fname:\n",
    "            channelType_local = 'extraLow'\n",
    "        else:\n",
    "            print(f\"Skipping {fname}: cannot determine channelType\")\n",
    "            continue\n",
    "\n",
    "        # Infer edfa_name\n",
    "        if 'preamp_' in fname:\n",
    "            edfa_name = fname.split('preamp_')[1].split('_')[0]\n",
    "        elif 'booster_' in fname:\n",
    "            edfa_name = fname.split('booster_')[1].split('_')[0]\n",
    "        else:\n",
    "            edfa_name = fname.split('_')[1] if len(fname.split('_')) > 1 else fname\n",
    "\n",
    "        features = featureExtraction_ML(\n",
    "            data['measurement_data'],\n",
    "            extractionType=extractionType_local,\n",
    "            channelType=channelType_local,\n",
    "            featureType=featureType,\n",
    "            # channelNum=data['measurement_setup'].get('roadm_wss_num_channel', 95)\n",
    "        )\n",
    "        features['edfa_index'] = edfa_name_to_index.get(edfa_name, -1)\n",
    "        features['EDFA_type'] = extractionType_local\n",
    "        # Move edfa_index to the first column\n",
    "        cols = features.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('EDFA_type')))\n",
    "        cols.insert(0, cols.pop(cols.index('edfa_index')))\n",
    "        features = features[cols]\n",
    "        all_features = pd.concat([all_features, features], ignore_index=True)\n",
    "        print(f\"Processed {fname}, extractionType: {extractionType_local}, channelType: {channelType_local}, edfa_name: {edfa_name}, edfa_index: {edfa_name_to_index.get(edfa_name, -1)}, rows: {len(features)}\")\n",
    "    return all_features\n",
    "\n",
    "# For aging train and test\n",
    "aging_train_features = extract_features_from_dir(\n",
    "    aging_train_dir, featureType='train'\n",
    ")\n",
    "aging_test_features = extract_features_from_dir(\n",
    "    aging_test_dir, featureType='test'\n",
    ")\n",
    "aging_train_features.to_csv(aging_output_csv_train, index=False)\n",
    "aging_test_features.to_csv(aging_output_csv_test, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc685cd9",
   "metadata": {},
   "source": [
    "unseen gain and tilt feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from libs.edfa_feature_extraction_libs import featureExtraction_ML\n",
    "\n",
    "unseen_train_dir = '../dataset/ML_challenge_user/Train/unseen'\n",
    "unseen_test_dir = '../dataset/ML_challenge_user/Test/unseen'\n",
    "\n",
    "unseen_output_csv_train = '../Features/Train/unseen/booster_fix_features_train.csv'\n",
    "unseen_output_csv_test = '../Features/Test/unseen/booster_goalpost_random_features_test.csv'\n",
    "\n",
    "# ---- HARDCODED MAPPING ----\n",
    "edfa_name_to_index = {\n",
    "    'rdm1-co1': 0,\n",
    "    'rdm2-co1': 1,\n",
    "    'rdm3-co1': 2,\n",
    "    'rdm4-co1': 3,\n",
    "    'rdm5-co1': 4,\n",
    "    'rdm6-co1': 5,\n",
    "    'rdm1-lg1': 6,\n",
    "    'rdm2-lg1': 7\n",
    "}\n",
    "print(\"EDFA name to index mapping (hardcoded):\")\n",
    "for name, idx in edfa_name_to_index.items():\n",
    "    print(f\"  {name}: {idx}\")\n",
    "\n",
    "def extract_features_from_dir(data_dir, featureType):\n",
    "    all_features = pd.DataFrame()\n",
    "    for fname in sorted(os.listdir(data_dir)):\n",
    "        if not fname.endswith('.json'):\n",
    "            continue\n",
    "        file_path = os.path.join(data_dir, fname)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if 'measurement_data' not in data or not data['measurement_data']:\n",
    "            print(f\"Skipping {fname}: no measurement_data\")\n",
    "            continue\n",
    "\n",
    "        # Infer extractionType\n",
    "        if 'preamp' in fname:\n",
    "            extractionType_local = 'preamp'\n",
    "        elif 'booster' in fname:\n",
    "            extractionType_local = 'booster'\n",
    "        else:\n",
    "            print(f\"Skipping {fname}: cannot determine extractionType\")\n",
    "            continue\n",
    "\n",
    "        # Improved channelType detection\n",
    "        if '_fix_' in fname or '_goalpost_' in fname:\n",
    "            channelType_local = 'fix'\n",
    "        elif '_random_' in fname:\n",
    "            channelType_local = 'random'\n",
    "        elif '_extraRandom_' in fname:\n",
    "            channelType_local = 'extraRandom'\n",
    "        elif '_extraLow_' in fname:\n",
    "            channelType_local = 'extraLow'\n",
    "        else:\n",
    "            print(f\"Skipping {fname}: cannot determine channelType\")\n",
    "            continue\n",
    "\n",
    "        # Infer edfa_name\n",
    "        if 'preamp_' in fname:\n",
    "            edfa_name = fname.split('preamp_')[1].split('_')[0]\n",
    "        elif 'booster_' in fname:\n",
    "            edfa_name = fname.split('booster_')[1].split('_')[0]\n",
    "        else:\n",
    "            edfa_name = fname.split('_')[1] if len(fname.split('_')) > 1 else fname\n",
    "\n",
    "        features = featureExtraction_ML(\n",
    "            data['measurement_data'],\n",
    "            extractionType=extractionType_local,\n",
    "            channelType=channelType_local,\n",
    "            featureType=featureType,\n",
    "            channelNum=data['measurement_setup'].get('roadm_wss_num_channel', 95)\n",
    "        )\n",
    "        features['edfa_index'] = edfa_name_to_index.get(edfa_name, -1)\n",
    "        features['EDFA_type'] = extractionType_local\n",
    "        # Move edfa_index to the first column\n",
    "        cols = features.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index('EDFA_type')))\n",
    "        cols.insert(0, cols.pop(cols.index('edfa_index')))\n",
    "        features = features[cols]\n",
    "        all_features = pd.concat([all_features, features], ignore_index=True)\n",
    "        print(f\"Processed {fname}, extractionType: {extractionType_local}, channelType: {channelType_local}, edfa_name: {edfa_name}, edfa_index: {edfa_name_to_index.get(edfa_name, -1)}, rows: {len(features)}\")\n",
    "    return all_features\n",
    "\n",
    "# For aging train and test\n",
    "unseen_train_features = extract_features_from_dir(\n",
    "    unseen_train_dir, featureType='train'\n",
    ")\n",
    "unseen_test_features = extract_features_from_dir(\n",
    "    unseen_test_dir, featureType='test'\n",
    ")\n",
    " \n",
    "unseen_train_features.to_csv(unseen_output_csv_train, index=False)\n",
    "unseen_test_features.to_csv(unseen_output_csv_test, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e5da5",
   "metadata": {},
   "source": [
    "COSMOS feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json,fnmatch\n",
    "import pandas as pd\n",
    "from libs.edfa_feature_extraction_libs import featureExtraction_ML\n",
    "\n",
    "data_prepath = '../dataset/COSMOS_EDFA_Dataset/'\n",
    "output_csv_aging_train = '../Features/Train/COSMOS/COSMOS_features.csv'\n",
    "\n",
    "# ---- HARDCODED MAPPING ----\n",
    "edfa_name_to_index = {\n",
    "    'rdm1-co1': 0,\n",
    "    'rdm2-co1': 1,\n",
    "    'rdm3-co1': 2,\n",
    "    'rdm4-co1': 3,\n",
    "    'rdm5-co1': 4,\n",
    "    'rdm6-co1': 5,\n",
    "    'rdm1-lg1': 6,\n",
    "    'rdm2-lg1': 7\n",
    "}\n",
    "print(\"EDFA name to index mapping (hardcoded):\")\n",
    "for name, idx in edfa_name_to_index.items():\n",
    "    print(f\"  {name}: {idx}\")\n",
    "\n",
    "fileList = ['rdm1-co1', 'rdm2-co1', 'rdm3-co1', 'rdm4-co1',\n",
    "            'rdm5-co1', 'rdm6-co1', 'rdm1-lg1', 'rdm2-lg1']\n",
    "folderList = ['fix', 'random', 'extraRandom', 'extraLow']\n",
    "edfaTypes = [\"booster\",\"preamp\"]\n",
    "gainList = [\"15dB\",\"18dB\",\"21dB\"]\n",
    "\n",
    "def matchFile(pattern, foler):\n",
    "    # match one file in the folder\n",
    "    # example usage:\n",
    "    # result = matchFile('*rdm1-co1*.json', '.../benchmark/extraRandom/')\n",
    "    # result is the full path \n",
    "    for file in os.listdir(foler):\n",
    "        if fnmatch.fnmatch(file, pattern):\n",
    "            return os.path.join(foler, file)\n",
    "            \n",
    "def extract_features_from_dir(data_dir, featureType,edfaType):\n",
    "    all_features = pd.DataFrame()\n",
    "    for fileName in fileList:\n",
    "        for channelType in folderList:\n",
    "            for gain in gainList:\n",
    "                filePath = data_prepath + edfaType + \"/\" + gain + \"/\" + channelType + \"/\"\n",
    "                fname = matchFile('*'+fileName+'*.json', filePath)\n",
    "                # file_path = os.path.join(data_dir, fname)\n",
    "                with open(fname, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                if 'measurement_data' not in data or not data['measurement_data']:\n",
    "                    print(f\"Skipping {fname}: no measurement_data\")\n",
    "                    continue\n",
    "\n",
    "                # Infer extractionType\n",
    "                if 'preamp' in fname:\n",
    "                    extractionType_local = 'preamp'\n",
    "                elif 'booster' in fname:\n",
    "                    extractionType_local = 'booster'\n",
    "                else:\n",
    "                    print(f\"Skipping {fname}: cannot determine extractionType\")\n",
    "                    continue\n",
    "\n",
    "                # Infer edfa_name\n",
    "                edfa_name = fileName\n",
    "                # if 'preamp_' in fname:\n",
    "                #     edfa_name = fname.split('preamp_')[1].split('_')[0]\n",
    "                # elif 'booster_' in fname:\n",
    "                #     edfa_name = fname.split('booster_')[1].split('_')[0]\n",
    "                # else:\n",
    "                #     edfa_name = fname.split('_')[1] if len(fname.split('_')) > 1 else fname\n",
    "\n",
    "                features = featureExtraction_ML(\n",
    "                    data['measurement_data'],\n",
    "                    extractionType=extractionType_local,\n",
    "                    channelType=channelType,\n",
    "                    featureType=featureType,\n",
    "                    channelNum=data['measurement_setup'].get('roadm_wss_num_channel', 95)\n",
    "                )\n",
    "\n",
    "                features['edfa_index'] = edfa_name_to_index.get(edfa_name, -1)\n",
    "                features['EDFA_type'] = extractionType_local\n",
    "                # Move edfa_index to the first column\n",
    "                cols = features.columns.tolist()\n",
    "                cols.insert(0, cols.pop(cols.index('EDFA_type')))\n",
    "                cols.insert(0, cols.pop(cols.index('edfa_index')))\n",
    "                features = features[cols]\n",
    "                all_features = pd.concat([all_features, features], ignore_index=True)\n",
    "                print(f\"Processed {fname}, extractionType: {extractionType_local}, channelType: {channelType}, edfa_name: {edfa_name}, edfa_index: {edfa_name_to_index.get(edfa_name, -1)}, rows: {len(features)}\")\n",
    "    return all_features\n",
    "\n",
    "combined_dfs = []\n",
    "# For aging train and test\n",
    "# edfaTypes = [\"preamp\"]\n",
    "for edfaType in edfaTypes: # \n",
    "    aging_train_features = extract_features_from_dir(\n",
    "        data_prepath, featureType='train', edfaType=edfaType\n",
    "    )\n",
    "    # aging_test_features = extract_features_from_dir(\n",
    "    #     aging_test_dir, featureType='test'\n",
    "    # )\n",
    "    # aging_testGround_features = extract_features_from_dir(\n",
    "    #     aging_testGround_dir, featureType='test_ground_truth'\n",
    "    # )   \n",
    "    # aging_train_features.to_csv(output_csv_aging_train.replace('COSMOS_', edfaType+\"_\"), index=False)\n",
    "    combined_dfs.append(aging_train_features)\n",
    "# aging_test_features.to_csv(output_csv_aging_test, index=False)\n",
    "# aging_testGround_features.to_csv(output_csv_aging_testGround, index=False)\n",
    "\n",
    "combined = pd.concat(combined_dfs, ignore_index=True)\n",
    "combined.to_csv(output_csv_aging_train, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9946f",
   "metadata": {},
   "source": [
    "split whole training files into train + test files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def split_csv_features_labels(input_folder, output_folder_features, output_folder_labels):\n",
    "    os.makedirs(output_folder_features, exist_ok=True)\n",
    "    os.makedirs(output_folder_labels, exist_ok=True)\n",
    "    for fname in os.listdir(input_folder):\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        fpath = os.path.join(input_folder, fname)\n",
    "        df = pd.read_csv(fpath)\n",
    "        # Columns containing 'gain' (case-insensitive)\n",
    "        # Find columns for WSS activated channel index and calculated gain spectra\n",
    "        wss_cols = [col for col in df.columns if 'dut_wss_activated_channel_index' in col.lower()]\n",
    "\n",
    "        label_cols = [col for col in df.columns if 'calculated_gain_spectra' in col.lower()]\n",
    "        feature_cols = [col for col in df.columns if col not in label_cols]\n",
    "        # Ensure the columns are sorted in the same order for elementwise multiplication\n",
    "        label_cols_sorted = sorted(label_cols)\n",
    "        wss_cols_sorted = sorted(wss_cols)\n",
    "        # Check that the number of label and wss columns match for elementwise multiplication\n",
    "        if len(label_cols_sorted) != len(wss_cols_sorted):\n",
    "            raise ValueError(f\"Number of label columns ({len(label_cols_sorted)}) does not match number of WSS columns ({len(wss_cols_sorted)}) in file {fname}\")\n",
    "        # Elementwise multiply the label columns by the WSS columns row-wise\n",
    "        \n",
    "        # Set label to np.nan where WSS is 0, keep label where WSS is 1\n",
    "        mask = df[wss_cols_sorted].values == 1\n",
    "        df[label_cols_sorted] = np.where(mask, df[label_cols_sorted].values, np.nan)\n",
    "        # Save\n",
    "        df[feature_cols].to_csv(os.path.join(output_folder_features, fname.replace('.csv', '_features.csv')), index=False)\n",
    "        df[label_cols_sorted].to_csv(os.path.join(output_folder_labels, fname.replace('.csv', '_labels.csv')), index=False)\n",
    "        print(f\"Processed {fname}: {len(feature_cols)} features, {len(label_cols)} labels.\")\n",
    "\n",
    "# All combinations for Train, shb, unseen, aging\n",
    "train_folders = [\n",
    "    ('../Features/Train/shb', '../Features/Train/shb/features', '../Features/Train/shb/labels'),\n",
    "    ('../Features/Train/unseen', '../Features/Train/unseen/features', '../Features/Train/unseen/labels'),\n",
    "    ('../Features/Train/aging', '../Features/Train/aging/features', '../Features/Train/aging/labels'),\n",
    "    ('../Features/Train/COSMOS', '../Features/Train/COSMOS/features', '../Features/Train/COSMOS/labels')\n",
    "]\n",
    "\n",
    "for input_folder, output_features, output_labels in train_folders:\n",
    "    if os.path.exists(input_folder):\n",
    "        split_csv_features_labels(input_folder, output_features, output_labels)\n",
    "    else:\n",
    "        print(f\"[WARNING] Input folder does not exist: {input_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# Move all *_features.csv files from the first folder to the second folder for each tuple in folders\n",
    "\n",
    "test_move_folder = [ \n",
    "    ('../Features/Test/shb', '../Features/Test/shb/features'),\n",
    "    ('../Features/Test/unseen', '../Features/Test/unseen/features'),\n",
    "    ('../Features/Test/aging', '../Features/Test/aging/features'),\n",
    "]\n",
    "\n",
    "for src_folder, dst_folder in test_move_folder: \n",
    "    # os.makedirs(dst_folder, exist_ok=True)\n",
    "    for fname in os.listdir(src_folder):\n",
    "        if fname.endswith('.csv'):\n",
    "            # Rename to _features.csv if not already\n",
    "            if not fname.endswith('_features.csv'):\n",
    "                base, _ = os.path.splitext(fname)\n",
    "                new_fname = base + '_features.csv'\n",
    "            else:\n",
    "                new_fname = fname\n",
    "            src_path = os.path.join(src_folder, fname)\n",
    "            dst_path = os.path.join(dst_folder, fname)\n",
    "            shutil.move(src_path, dst_path)\n",
    "            print(f\"Moved {src_path} -> {dst_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665cbee",
   "metadata": {},
   "source": [
    "Combine all training and test set together and convert it into kaggle style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csvs_with_id_usage(input_dirs, output_csv, add_id_usage=False, add_category=True):\n",
    "    \"\"\"\n",
    "    Combine CSVs from input_dirs, optionally add ID and Usage columns, \n",
    "    and save to output_csv.\n",
    "    \"\"\"\n",
    "    combined = []\n",
    "    for input_dir in input_dirs:\n",
    "        category = os.path.basename(os.path.dirname(input_dir))  # e.g. \"aging\", \"shb\", \"unseen\"\n",
    "        csv_files = sorted(glob.glob(os.path.join(input_dir, \"*.csv\")))\n",
    "        for csv_file in csv_files:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df.fillna(0, inplace=True)\n",
    "            n_rows = df.shape[0]\n",
    "\n",
    "            # Add optional ID & Usage\n",
    "            if add_id_usage:\n",
    "                half = n_rows // 2\n",
    "                usage_col = [\"Public\"] * half + [\"Private\"] * (n_rows - half)\n",
    "                df.insert(0, \"Usage\", usage_col)\n",
    "                if not hasattr(combine_csvs_with_id_usage, \"global_id_counter\"):\n",
    "                    combine_csvs_with_id_usage.global_id_counter = 1\n",
    "                start_id = combine_csvs_with_id_usage.global_id_counter\n",
    "                df.insert(0, \"ID\", list(range(start_id, start_id + n_rows)))\n",
    "                combine_csvs_with_id_usage.global_id_counter += n_rows\n",
    "\n",
    "            if add_category:\n",
    "                df.insert(2 if add_id_usage else 0, \"Category\", [category] * n_rows)\n",
    "                \n",
    "            combined.append(df)\n",
    "\n",
    "    if combined:\n",
    "        final_df = pd.concat(combined, ignore_index=True)\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Combined CSV saved to {output_csv} with {final_df.shape[0]} rows.\")\n",
    "    else:\n",
    "        print(f\"No CSVs found in {input_dirs}\")\n",
    "\n",
    "    # Reset counter for next call\n",
    "    if hasattr(combine_csvs_with_id_usage, \"global_id_counter\"):\n",
    "        delattr(combine_csvs_with_id_usage, \"global_id_counter\")\n",
    "\n",
    "# Define input directories for features and labels\n",
    "# Combine feature directories for Train\n",
    "train_feature_dirs = [\n",
    "    '../Features/Train/aging/features',\n",
    "    '../Features/Train/shb/features',\n",
    "    '../Features/Train/unseen/features',\n",
    "    # '../Features/Train/COSMOS/features'\n",
    "]\n",
    "\n",
    "train_label_dirs = [\n",
    "    '../Features/Train/aging/labels',\n",
    "    '../Features/Train/shb/labels',\n",
    "    '../Features/Train/unseen/labels',\n",
    "    # '../Features/Train/COSMOS/labels'\n",
    "]\n",
    "\n",
    "# Combine features (all Usage = Public)\n",
    "combine_csvs_with_id_usage(\n",
    "    train_feature_dirs,\n",
    "    \"../Features/Train/train_features.csv\", \n",
    "\tadd_id_usage=False,\n",
    "    add_category=True\n",
    ")\n",
    "\n",
    "combine_csvs_with_id_usage(\n",
    "    train_label_dirs,\n",
    "    \"../Features/Train/train_labels.csv\",\n",
    "    add_id_usage=False,\n",
    "    add_category=False\n",
    ")\n",
    "\n",
    "test_feature_dirs = [\n",
    "    '../Features/Test/aging/features',\n",
    "    '../Features/Test/shb/features',\n",
    "    '../Features/Test/unseen/features'\n",
    "]\n",
    "\n",
    "combine_csvs_with_id_usage(\n",
    "    test_feature_dirs,\n",
    "    \"../Features/Test/test_features.csv\", \n",
    "\tadd_id_usage=True,\n",
    "    add_category=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58474f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COSMOS_feature_dirs = [\n",
    "    '../Features/Train/COSMOS/features'\n",
    "]\n",
    "\n",
    "COSMOS_label_dirs = [\n",
    "    '../Features/Train/COSMOS/labels'\n",
    "]\n",
    "\n",
    "combine_csvs_with_id_usage(\n",
    "    COSMOS_feature_dirs,\n",
    "    \"../Features/Train/COSMOS_features.csv\", \n",
    "\tadd_id_usage=False,\n",
    "    add_category=True\n",
    ")\n",
    "\n",
    "combine_csvs_with_id_usage(\n",
    "    COSMOS_label_dirs,\n",
    "    \"../Features/Train/COSMOS_labels.csv\",\n",
    "    add_id_usage=False,\n",
    "    add_category=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
